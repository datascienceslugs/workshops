{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing a model for the Titanic competition\n",
    "*Anders Poirel - 14-10-2019*\n",
    "\n",
    "We've seen how to prepare the data for the Titanic dataset and are pready to fit some models. \n",
    "Import the data we pre-processed last time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train = pd.read_csv('train_clean.csv')\n",
    "test = pd.read_csv('test_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You already have a clean dataset but that doesn't mean that there is not some data pre-processing to be done. You won't be using the `Name`, `Ticket` and `Title` features. You need to create dummy variables for `Sex`. Finally, while you might not be interested in the specific cabin number, whether a passenger has a cabin or not should be important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_cabin(x):\n",
    "    if pd.isna(x):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def pre_process(dataset):\n",
    "    dataset = pd.get_dummies(dataset, columns = ['Sex'], drop_first = True)\n",
    "    dataset['HasCabin'] = dataset['Cabin'].apply(has_cabin)\n",
    "    dataset.drop(['Embarked', 'Cabin', 'Name', 'Ticket', 'Title'], inplace = True, axis = 1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pre_process(train)\n",
    "X_test = pre_process(test)\n",
    "\n",
    "y_train = train['Survived']\n",
    "X_train = train.drop('Survived', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We adapt regression so that the values fall between \\[0,1\\], and interpret the values as the probability that the point should be classified in the positive category:\n",
    "One such function is the logistic function:\n",
    "$$p(X) = \\frac{e^{\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_n X_n}}{1 + e^{\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_n X_n}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then seek to maximize the likelyhood function:\n",
    "$$(\\beta_0, \\ldots, \\beta_1) = argmax_{\\beta_0, \\ldots, \\beta_1} \\prod_{i: y_i =1}P(x_i) \\prod_{i: y_i = 0}(1-P(x_i))$$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make predictions, we classify points with predicted probabilities <0.5 as 0 and those with over 0.5 as 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following syntax can be reused across most of the `sklearn` and `keras` APIs (this is the `estimator` interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = LogisticRegression()\n",
    "model_1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the performance of our model we will be using the accuracy metric. As a reminder, the accuracy of an estimate $\\mathbf{\\hat{y}}$ on a classification task is defined as:\n",
    "$$Accuracy(\\mathbf{y}, \\mathbf{\\hat{y}}) = \\frac{1}{n}\\sum_{i =1}^{n} \\mathbb{1}(y_i = \\hat{y_i})$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "accuracy(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the documentation for the API [here](https://scikit-learn.org/stable/modules/tree.html). \n",
    "Refer to ISL [2] for more information on decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before defining and fitting the model, you will want to read up on the different hyperparameters the model takes on the scikit-learn documentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try fitting the model on your own and making predictions\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try to get a better estimate of how the model performs using cross-validation -which you may remember from our basic statistics workshop, otherwise refer to ISL [3]-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(model2, X_train, y_train, cv = 10, scoring = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this to compare the perfomance of your decision tree and logistic regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could also try playing around with the hyperparameters of the model to see if you can improve the cross-validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting decision tree can be visualized as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export(model2, out_file = None)\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between model parameters and hyper-parameters\n",
    "Parameters are the values that are calculated during model training, e.g. the weights $\\beta_0, \\ldots, \\beta_n$ in logistic regression. \n",
    "\n",
    "Hyper-parameters on the other hand, are values that determine how exactly an training algorithm is carried out, for instance the maximum depth of a decision tree, or the number of features the decision tree is allowed to consider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier, you might have tried to improve you model by modifying its hyper-parameters. A more systematic of doing so it to specify a set of hyperparameters to explore, and find the best performing ones by cross-validating the model for each combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {'criterion' : ['gini', 'entropy'],\n",
    "          'max_depth' : [3,5,10],\n",
    "          'max_features' : ['sqrt', 'log2'],\n",
    "          'min_samples_leaf' : [1,2,5]\n",
    "          }\n",
    "tuned_model = GridSearchCV(estimator = DecisionTreeClassifier(),\n",
    "                           scoring = 'accuracy',\n",
    "                           param_grid = params,\n",
    "                           cv = 10,\n",
    "                           n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the best set of parameters after a grid search as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now use these parameters to fit a definitive model on the entire dataset. Alternatively, you could also use them as the starting point of a new grid search to see if you can eke out even more performance.\n",
    "\n",
    "You can see how well the best set of parameters performs using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to the score on the untuned model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests & Ensemble Models\n",
    "\n",
    "Find the documention for Random Forests here [here]()\n",
    "You can read more about the theory behind the random forest algorithm in ISL [4]. A random forest is an ensemble model, which means that we are averaging a bunch of simpler models.\n",
    "\n",
    "Here, we build a number of decision trees slightly differently and then average them by majority voting, i.e. we classify a training instance according to how the majority of decision trees we built would classify it. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try tuning a random forest model on your own\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] *An Introduction to Statistical Learning*, 4.3 Logistic Regression, pp. 130-137\n",
    "\n",
    "[2] *An Introduction to Statistical Learning*, 8.1 The Basics of Decision Trees, pp.303-314\n",
    "\n",
    "[3] *An Introduction to Statistical Learning*, 5.1 Cross-Validation pp. 176-183\n",
    "\n",
    "[4] *An Introduction to Statistical Learning*, 8.2.1 Bagging & 8.2.3 Random Forests, pp 316-320"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
